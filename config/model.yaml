# Model configuration
num_layers: 2
hidden_dim: 256
num_heads: 8
head_dim: 128
rope_dim: 128
kv_compression_dim: 64
query_compression_dim: 64 
num_experts: 2
activated_experts: 2
vocab_size : 50258 # Wont be necessary here. Since we may use different tokenizers in during data preparation
min_seq_len: 5
max_seq_len: 64 
stride: 8

