# Optimized training configuration
batch_size: 16 
learning_rate: 0.00012345
num_epochs: 200 

# Optimizer settings
optimizer: adamw  # Changed from sgd to adamw
weight_decay: 0.01

# Scheduler settings
scheduler: linear_warmup  
min_lr: 0.00001
warmup_steps: 100  

# Training settings
gradient_clip: 1.0
label_smoothing: 0.1
use_augmentations: False
gradient_accumulation_steps: 3  # Reduced from 10
patience: 10 # Early stopping if no progress for number of epochs
eval_steps: 1000 
mtp_weight: 0.1  