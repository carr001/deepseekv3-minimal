# Training configuration for DeepSeek model
batch_size: 8
learning_rate: 0.00012345
num_epochs: 500

# Optimizer settings
optimizer: sgd  # Options: sgd, adamw
momentum: 0.9   # For SGD optimizer
nesterov: true  # For SGD optimizer

# Scheduler settings
scheduler: cosine_warmup  # Options: cosine, cosine_warmup, linear_warmup
min_lr: 0.00001  # Minimum learning rate for cosine annealing
restart_epochs: 10  # For cosine_warmup, T_0 parameter
restart_mult: 2    # For cosine_warmup, T_mult parameter
warmup_steps: 500  # For linear_warmup scheduler

# Training settings
gradient_clip: 1.0
label_smoothing: 0.15
use_augmentations: False
gradient_accumulation_steps: 10
patience: 20  # Early stopping patience
eval_steps: 250  # Validation frequency in steps
mtp_weight: 0.25  # Weight for MTP loss component
