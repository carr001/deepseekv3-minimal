# Training configuration for DeepSeek model
batch_size: 16
learning_rate: 0.0012345
num_epochs: 200

# Optimizer settings
optimizer: sgd  # Options: sgd, adamw
momentum: 0.9   # For SGD optimizer
nesterov: true  # For SGD optimizer

# Scheduler settings
scheduler: cosine_warmup  # Options: cosine, cosine_warmup, linear_warmup
min_lr: 0.0001  # Minimum learning rate for cosine annealing
restart_epochs: 10  # For cosine_warmup, T_0 parameter
restart_mult: 2    # For cosine_warmup, T_mult parameter
warmup_steps: 500  # For linear_warmup scheduler

# Training settings
gradient_clip: 0.5
label_smoothing: 0.15
use_augmentations: False
gradient_accumulation_steps: 2
patience: 10  # Early stopping patience
eval_steps: 70  # Validation frequency in steps
eval_nth_epoch: 10  # Validation frequency in epochs
mtp_weight: 0.2  # Weight for MTP loss component
