# Training configuration
batch_size: 24
learning_rate: 0.0002345
num_epochs: 100
gradient_clip: 2.0
warmup_steps: 1000
use_augmentations: False
gradient_accumulation_steps: 4  
patience: 7  # Add early stopping patience
eval_steps: 500  
mtp_weight: 0.4  # Weight for MTP loss component